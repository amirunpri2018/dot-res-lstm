{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Input\n",
    "from keras.layers import Conv1D, Flatten, Lambda\n",
    "from keras.layers import Reshape, UpSampling1D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all = np.load('x_sigs.npy')\n",
    "fx = np.load('features.npy')\n",
    "y_b = np.load('y_mc.npy')\n",
    "\n",
    "# binary class labels\n",
    "# for later cases, using only class 0 for 0, and all other classes as 1\n",
    "y_b[y_b<=0] = 0\n",
    "y_b[y_b>=1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 905)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_aug = np.load('Xs_aug.npy')\n",
    "fx_aug = np.load('Xf_aug.npy')\n",
    "y_aug = np.load('y_aug.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 905, 1)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_aug.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_aug = x_aug[-50:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_aug = np.reshape(x_aug, (50, 905))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 905)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_aug.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 10)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fx_aug.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx_aug = fx_aug[-50:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 10)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fx_aug.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800,)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_aug.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_aug = y_aug[-50:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_aug.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all = np.concatenate((x_all, x_aug), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1050, 905)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx = np.concatenate((fx, fx_aug), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1050, 10)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_b = np.concatenate((y_b, y_aug), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1050,)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.25362262e+09 5.05698927e+04 1.83679572e+03 2.69776884e+04\n",
      " 1.51470377e-01 2.32846075e+00 1.54156457e+00 1.79779301e+06\n",
      " 2.17585754e+04 2.98425756e+01]\n",
      "-5.413658939124573e-17\n",
      "1.0000000000000002\n",
      "[1847.18300896 1847.37135325 1847.05877853 1847.24672399 1847.51090507\n",
      " 1846.27153128 1843.77220248 1842.88928133 1840.97111381 1837.53809794\n",
      " 1835.88317836 1835.35883729 1832.89841928 1836.32703679 1840.62771912\n",
      " 1843.42540188 1848.37911154 1848.11294929 1845.75746079 1845.83312391\n",
      " 1850.32407995 1855.98804317 1855.72918478 1852.30444526 1849.98728375\n",
      " 1849.64148369 1847.5087788  1843.24338446 1842.34087518 1838.28448109\n",
      " 1837.25375805 1843.49494536 1847.99723917 1846.05040262 1844.93333153\n",
      " 1844.55659326 1840.69982959 1839.41432551 1842.05633513 1842.74226129\n",
      " 1842.64586231 1845.12591015 1848.17425473 1850.33159461 1854.31089183\n",
      " 1852.69627288 1844.99283678 1839.96064784 1835.65085292 1834.97830959\n",
      " 1837.8362868  1843.62516996 1847.4733598  1846.74379073 1844.27704884\n",
      " 1844.04811878 1842.75671836 1840.22545243 1840.03381274 1839.41035479\n",
      " 1838.36407513 1837.45488363 1840.33176187 1845.39544594 1846.11669111\n",
      " 1841.94004842 1835.47475315 1830.11472108 1825.03007703 1829.55687068\n",
      " 1837.01353287 1835.39030726 1829.79180527 1831.12024737 1836.11256732\n",
      " 1837.8346568  1838.46224326 1841.17677095 1843.72844113 1845.21205365\n",
      " 1848.19578275 1844.75709241 1843.18028585 1846.11534871 1841.53472397\n",
      " 1838.38376281 1840.01189542 1840.04275894 1840.9636507  1836.66326887\n",
      " 1832.80861233 1836.51181159 1838.82383327 1839.94002881 1844.15758151\n",
      " 1846.69687167 1847.72986826 1844.58776777 1842.01590227 1843.40333255\n",
      " 1842.05133336 1841.48213469 1842.42186932 1838.12774432 1835.01237086\n",
      " 1834.21398456 1838.01194979 1841.00725116 1843.71479298 1842.72057443\n",
      " 1838.36894228 1837.29354637 1838.4919269  1836.09286474 1833.05126825\n",
      " 1836.03549223 1840.26318823 1844.0599533  1846.83809408 1846.50962752\n",
      " 1841.64610368 1838.23326598 1835.5778215  1833.85257183 1834.06300057\n",
      " 1836.51718527 1837.96653026 1841.02968379 1842.25113691 1836.87099787\n",
      " 1833.19118347 1833.13473999 1837.20266199 1838.69909315 1832.59729776\n",
      " 1828.48949499 1833.0345451  1836.69358969 1839.12179091 1836.92952404\n",
      " 1836.09909899 1839.47109634 1843.34236437 1844.06949484 1838.98525684\n",
      " 1836.62048617 1836.14650361 1835.75749399 1836.47044696 1835.57772786\n",
      " 1836.15540872 1837.66993389 1839.23515861 1837.57487879 1836.84159066\n",
      " 1836.1344641  1838.96842317 1847.75774923 1847.68056814 1843.17662247\n",
      " 1841.15702944 1843.30898434 1844.3202316  1842.25446828 1838.81393771\n",
      " 1835.53252633 1833.29735128 1833.38775135 1836.61104947 1840.53000928\n",
      " 1843.18346544 1844.8369907  1845.83551074 1845.2300692  1844.40793316\n",
      " 1843.21161401 1838.28593983 1837.47776369 1839.52318042 1838.43327283\n",
      " 1839.94780019 1840.77906587 1839.8445641  1839.15715897 1838.79277168\n",
      " 1837.58156184 1837.27374098 1837.50377194 1835.90019856 1838.30819572\n",
      " 1842.35768905 1842.69131463 1840.68877744 1839.75977027 1837.60010662\n",
      " 1834.61969653 1833.75234462 1834.3182051  1834.00458736 1834.21594083\n",
      " 1838.4824844  1844.80102215 1846.05216489 1843.00405616 1844.05938426\n",
      " 1842.73272573 1841.24965018 1842.33970135 1841.99490592 1841.02309385\n",
      " 1840.31009757 1839.19520933 1840.85240783 1842.20783963 1837.48750817\n",
      " 1834.76092758 1838.82396008 1841.67589153 1842.54851212 1840.8402116\n",
      " 1839.09373585 1840.16710331 1838.42646707 1838.04769648 1839.4004581\n",
      " 1839.64930812 1839.42978879 1838.05749286 1838.94725065 1838.31955641\n",
      " 1837.93495392 1842.2426414  1845.70208045 1846.85736426 1847.48503756\n",
      " 1842.27332213 1838.34289607 1835.73521949 1836.18441891 1837.73450912\n",
      " 1834.08636648 1829.7805901  1831.73401955 1834.34520303 1835.57089126\n",
      " 1835.75024509 1836.06225267 1842.2840988  1846.89134782 1848.35751581\n",
      " 1851.38244561 1851.95660904 1845.70030011 1839.71584483 1840.090245\n",
      " 1841.46174872 1845.87560407 1850.77312468 1849.05578901 1840.84921995\n",
      " 1838.36148464 1840.62299666 1841.23791953 1841.45567204 1840.84689252\n",
      " 1841.43640498 1844.36507571 1844.07714698 1846.62899502 1845.34411579\n",
      " 1838.69237407 1836.45767068 1834.51066527 1831.48982912 1832.640635\n",
      " 1838.81660429 1840.29328677 1838.27609259 1842.21241583 1845.15279323\n",
      " 1841.45938501 1841.59331511 1846.5879562  1847.61760036 1844.21893477\n",
      " 1842.08467944 1840.88602446 1835.48241754 1829.07055475 1828.12149266\n",
      " 1832.06765264 1836.90403541 1841.38789389 1842.23444752 1838.44708504\n",
      " 1837.58946258 1841.48628334 1844.92651536 1845.1749423  1843.77743914\n",
      " 1842.21498521 1837.32331504 1833.96633669 1835.70171374 1838.61567641\n",
      " 1840.45556961 1843.33404011 1845.57628644 1849.08878527 1851.84643293\n",
      " 1849.95195163 1846.32456343 1842.39455854 1838.05854706 1835.99812985\n",
      " 1837.96674132 1842.49695163 1842.54999784 1838.08913013 1835.43502769\n",
      " 1836.55925196 1840.01903148 1842.79084042 1847.10575857 1846.6186946\n",
      " 1845.40265215 1844.78833895 1846.58480483 1847.39825188 1850.52711177\n",
      " 1852.16779543 1847.42123442 1845.31477331 1845.32480911 1842.80278744\n",
      " 1840.22376726 1839.67895312 1845.0110827  1848.56964008 1842.58119055\n",
      " 1838.4551409  1842.04442984 1841.59764598 1840.66238166 1840.61243205\n",
      " 1839.97061172 1840.54454355 1841.5193717  1842.56791008 1845.83221255\n",
      " 1845.98614347 1848.40605565 1847.39974564 1841.33019943 1837.81970448\n",
      " 1839.0135707  1840.63785878 1842.72950735 1842.8996892  1844.21525942\n",
      " 1844.63683449 1844.47413692 1844.79646034 1845.73691663 1848.10770633\n",
      " 1849.46379851 1845.88708845 1843.11090269 1840.50674529 1840.44867245\n",
      " 1841.17388264 1841.62230483 1843.62704625 1847.46349746 1850.4533496\n",
      " 1847.24405044 1844.39727716 1846.47592181 1843.9956047  1842.21021877\n",
      " 1839.38857923 1837.57529032 1837.96719502 1839.03787198 1841.0262593\n",
      " 1841.92915818 1842.62707214 1844.62156385 1844.85174627 1844.70711151\n",
      " 1846.01842138 1846.64452987 1848.22809439 1847.05731039 1844.83051085\n",
      " 1845.25003369 1843.67188113 1842.32476387 1843.62775406 1845.95435212\n",
      " 1846.4906118  1847.13014428 1845.00666253 1846.19299821 1849.89380329\n",
      " 1849.2651586  1847.4881328  1847.03642496 1847.70043139 1847.45325176\n",
      " 1846.62609807 1848.44697323 1848.50000596 1845.17889353 1844.82530566\n",
      " 1842.70139596 1843.96058847 1846.60168995 1848.49282834 1846.47820026\n",
      " 1842.66858121 1842.55588255 1846.40603501 1849.09521456 1848.74825125\n",
      " 1845.92994549 1845.83685799 1846.67340535 1844.19125765 1838.92213616\n",
      " 1836.69094427 1837.76242306 1838.07402325 1838.69067613 1842.0318982\n",
      " 1842.25903914 1839.90250859 1841.83763922 1842.75488474 1843.87164356\n",
      " 1844.55495786 1844.19771318 1840.87131598 1838.61263883 1837.59767619\n",
      " 1838.99843728 1839.96526991 1843.36849265 1842.11598502 1836.50405092\n",
      " 1834.37524773 1837.90868078 1841.88681632 1840.97738338 1843.9191755\n",
      " 1849.30131968 1850.22129802 1848.37297928 1846.22744301 1845.94175253\n",
      " 1848.601422   1850.31796328 1846.25178671 1841.18345143 1838.70667499\n",
      " 1839.40478611 1841.7583662  1843.20549401 1844.02879808 1843.1647958\n",
      " 1844.41301838 1847.68596719 1844.62918048 1839.53034217 1838.02241472\n",
      " 1838.98976287 1842.01060945 1847.33664212 1847.6174721  1845.54729425\n",
      " 1845.97217245 1846.9350045  1851.49415292 1854.02844504 1849.55113145\n",
      " 1846.13395872 1848.15407935 1851.95657547 1849.45933276 1843.46543623\n",
      " 1841.79172944 1843.29665416 1842.9778889  1842.91758679 1842.49356159\n",
      " 1842.05062245 1839.38619189 1837.67778464 1838.16836466 1838.62120697\n",
      " 1839.2136178  1843.41803696 1850.08082615 1848.65920558 1840.52796808\n",
      " 1837.66415107 1839.32919841 1837.59058207 1837.18619871 1835.53199558\n",
      " 1836.59971509 1838.31775853 1843.55028554 1845.39591956 1843.39916804\n",
      " 1841.7672179  1841.25175747 1842.33333728 1845.55941929 1844.16756983\n",
      " 1844.15558126 1843.86347898 1841.76060744 1841.03272093 1846.3899925\n",
      " 1848.66043516 1847.65535569 1844.9933625  1846.32547188 1844.70313973\n",
      " 1842.24916538 1843.99654616 1844.86134735 1845.88716563 1846.38236544\n",
      " 1848.39954852 1849.17024704 1846.06226562 1842.45352634 1841.79435692\n",
      " 1843.60443695 1845.13785718 1844.10066506 1842.95393576 1841.48350615\n",
      " 1837.98033772 1837.93852441 1841.32660653 1841.0837775  1842.66022735\n",
      " 1844.22624895 1843.63353296 1843.50945215 1841.62497619 1844.83728139\n",
      " 1846.44329813 1845.39322195 1844.49937667 1843.83562354 1845.43529756\n",
      " 1846.25095133 1845.40031497 1842.6978992  1841.50351262 1843.23961539\n",
      " 1841.95482201 1837.48332362 1835.34704942 1836.37291973 1840.1079436\n",
      " 1842.91126907 1844.19773208 1845.92957339 1847.53934152 1845.95696982\n",
      " 1845.86313564 1848.48861622 1846.57413519 1845.2963765  1843.64923667\n",
      " 1845.53464057 1847.09732402 1848.26265069 1848.42479297 1846.44497521\n",
      " 1846.06147629 1844.06351441 1839.57384526 1835.18793349 1834.78583111\n",
      " 1836.46854181 1837.69519109 1841.81544533 1847.2289682  1852.32777904\n",
      " 1851.75191551 1848.21553748 1845.74633921 1850.06051479 1850.81673513\n",
      " 1842.1286185  1839.32133118 1843.67723116 1845.26021749 1841.98073538\n",
      " 1840.19841291 1841.2116689  1841.4313898  1838.13760772 1837.74918089\n",
      " 1840.07322193 1844.67961178 1846.6918774  1845.9888738  1849.06044252\n",
      " 1851.37980028 1847.68061356 1844.537914   1843.74831023 1845.31122732\n",
      " 1845.00960117 1842.36369166 1841.06383286 1839.73551513 1834.73343001\n",
      " 1836.30548509 1839.0912681  1836.59091523 1834.03588818 1834.8803956\n",
      " 1835.14564492 1837.1543087  1838.36923818 1840.63845803 1845.078418\n",
      " 1845.51126346 1846.3455669  1847.47819891 1849.44220582 1845.11592465\n",
      " 1839.76607186 1841.84405613 1848.1115762  1850.61423272 1847.18353097\n",
      " 1842.63646017 1839.47015143 1838.69410375 1841.47085689 1838.87714129\n",
      " 1835.46609679 1834.40493386 1833.83949576 1833.32281041 1837.74063659\n",
      " 1839.42736724 1839.78622387 1840.1340638  1840.66412273 1842.41877732\n",
      " 1840.78660653 1838.69215246 1839.07840267 1838.20428883 1834.96594672\n",
      " 1835.60357769 1838.68179823 1841.18763731 1843.04263168 1842.65183875\n",
      " 1843.77866233 1843.17042554 1842.85617302 1841.22979715 1837.12711958\n",
      " 1836.70651687 1838.7250991  1838.65075893 1840.17923591 1842.93645629\n",
      " 1845.45428533 1842.92298165 1839.65492752 1843.37690038 1847.05534167\n",
      " 1845.90320581 1844.12228123 1843.90714274 1845.91610191 1846.16963474\n",
      " 1846.34897081 1842.66869627 1840.21289702 1839.90999895 1838.55961365\n",
      " 1839.36250035 1839.60706673 1837.50544448 1839.31810585 1843.79804223\n",
      " 1845.43268614 1841.55662806 1840.2004329  1842.6693931  1845.28257528\n",
      " 1843.36170034 1842.39850773 1840.32366533 1839.93129038 1838.93338122\n",
      " 1841.86585857 1846.53814048 1848.72760727 1843.74020925 1837.54094853\n",
      " 1836.06738496 1840.03266332 1841.61354004 1840.67228043 1841.78004282\n",
      " 1843.94098735 1846.65453775 1848.1442771  1846.55026291 1844.12542155\n",
      " 1840.88873082 1836.93908999 1836.54170566 1837.63584221 1838.07729932\n",
      " 1837.963252   1839.66393447 1843.8599005  1848.696698   1853.35993221\n",
      " 1853.88662153 1849.54322568 1846.21794339 1843.93240482 1841.62800278\n",
      " 1839.84951134 1838.32216971 1837.52424138 1840.5342775  1843.084544\n",
      " 1841.24416064 1840.84187267 1842.48441947 1844.41559102 1843.08218419\n",
      " 1840.43922212 1836.50751339 1835.20697169 1838.25345477 1842.11236782\n",
      " 1843.72605244 1841.88059473 1839.25162378 1839.83360866 1841.3947946\n",
      " 1844.00724121 1844.76852691 1840.42375106 1840.65630751 1844.82120469\n",
      " 1842.77913567 1840.92362569 1840.01320003 1839.47999093 1838.20130506\n",
      " 1837.62301894 1841.22889941 1844.02974187 1842.53027766 1842.53084953\n",
      " 1844.26795552 1843.88941334 1844.22106073 1844.76790257 1840.55290105\n",
      " 1837.87559432 1838.9450567  1843.70395554 1847.69638164 1846.56572227\n",
      " 1846.73121372 1851.20756188 1853.37420776 1850.90033311 1851.52845948\n",
      " 1854.35956328 1851.04187281 1847.16376716 1846.69269293 1845.42290849\n",
      " 1844.66173182 1841.84864011 1841.81326794 1841.69156345 1839.91401446\n",
      " 1835.72030154 1836.93275421 1841.5490045  1846.19528825 1851.99978007\n",
      " 1852.19050086 1848.5256068  1845.68543819 1844.72726482 1842.72581927\n",
      " 1843.08008211 1844.43599087 1844.1421276  1839.34773241 1836.17346688\n",
      " 1837.83469889 1837.93870819 1836.95345973 1840.2793402  1844.11328835\n",
      " 1842.71053032 1841.69000114 1840.3344085  1833.84197973 1832.15580558\n",
      " 1833.66918759 1837.53349158 1842.2953069  1842.85504942 1842.48127142\n",
      " 1839.93746578 1838.27762692 1840.35712996 1841.02790721 1839.07622095\n",
      " 1842.46012257 1843.30945948 1839.234796   1834.87862906 1836.42770736\n",
      " 1838.18763244 1834.82167018 1831.74970646 1832.54191837 1836.63827197\n",
      " 1841.78401201 1840.85857689 1842.33159652 1843.87596471 1839.4796705\n",
      " 1838.14739258 1843.22597647 1845.5307958  1845.02967427 1844.01693191\n",
      " 1846.44657238 1849.93007706 1849.21958127 1848.43854568 1850.56152571\n",
      " 1851.05154943 1847.49373237 1848.09868173 1847.50971322 1844.66706064\n",
      " 1842.26405172 1838.01509572 1837.07469605 1839.40039457 1841.31604596\n",
      " 1841.47850662 1839.15253073 1837.32685548 1836.93437552 1837.40874259\n",
      " 1840.91987389 1845.8258899  1843.6156774  1842.62734355 1842.42186104\n",
      " 1841.5421589  1843.04229388 1846.18080541 1845.93352952 1841.22289219\n",
      " 1841.11003879 1845.20292826 1845.2565954  1841.70263827 1840.48123442\n",
      " 1842.07839762 1844.24314159 1841.49706862 1839.30941427 1841.34487909\n",
      " 1842.86704144 1841.83248691 1840.27406627 1842.92936217 1845.64337899\n",
      " 1846.96875694 1849.03157779 1850.62671046 1851.63961187 1851.85074558]\n",
      "8.22152e-09\n",
      "0.99999994\n",
      "StratifiedShuffleSplit(n_splits=1, random_state=0, test_size=0.2,\n",
      "            train_size=None)\n",
      "(840, 905)\n",
      "(840, 10)\n",
      "(210, 905)\n",
      "(210, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(fx)\n",
    "print(scaler.mean_)\n",
    "fx = scaler.transform(fx)\n",
    "\n",
    "print(np.mean(fx))\n",
    "print(np.std(fx))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_all)\n",
    "print(scaler.mean_)\n",
    "x_all = scaler.transform(x_all)\n",
    "\n",
    "print(np.mean(x_all))\n",
    "print(np.std(x_all))\n",
    "\n",
    "# train, test  == stratified\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "sss.get_n_splits(fx, y_b)\n",
    "\n",
    "print(sss)       \n",
    "\n",
    "for train_index, test_index in sss.split(fx, y_b):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    Xs_train, Xf_train, Xs_test, Xf_test = x_all[train_index], fx[train_index], x_all[test_index], fx[test_index]\n",
    "    y_train, y_test = y_b[train_index], y_b[test_index]\n",
    "    \n",
    "    \n",
    "print(Xs_train.shape)\n",
    "print(Xf_train.shape)\n",
    "print(Xs_test.shape)\n",
    "print(Xf_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(840, 905, 1)\n",
      "(210, 905, 1)\n"
     ]
    }
   ],
   "source": [
    "Xs_train = np.reshape(Xs_train, (840,905,1))\n",
    "print(Xs_train.shape)\n",
    "\n",
    "Xs_test = np.reshape(Xs_test, (210,905,1))\n",
    "print(Xs_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train, 2)\n",
    "y_test = to_categorical(y_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def step_decay(epoch, triangular = False):\n",
    "    initial_lrate = 0.1\n",
    "    drop = 0.7\n",
    "    epochs_drop = 3.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    if triangular == True and epoch%2 == 1:\n",
    "        lrate += lrate\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y = [step_decay(x) for x in range(40)]\n",
    "#print(y)\n",
    "plt.plot(y, '.-')\n",
    "#plt.plot(y_tri, '.-k')\n",
    "plt.title('Learning rate scheduling')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('learning rate')\n",
    "plt.legend(['step decay'])\n",
    "plt.savefig('learning_rate.png', dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import LSTM, CuDNNLSTM, Dense, Dropout, Input, ConvLSTM2D, Flatten, Add, Concatenate, Dot, Multiply\n",
    "from keras.layers import Maximum, Average, Activation\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "import math\n",
    "\n",
    "\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "\n",
    "neurons = (2<<5) - 1\n",
    "\n",
    "t_len = 905\n",
    "n_class = 2\n",
    "\n",
    "\n",
    "x_i = Input(shape = (905,1))\n",
    "x_i2 = Input(shape = (10,))\n",
    "\n",
    "# bf\n",
    "x = CuDNNLSTM(units=neurons, return_sequences = True, kernel_initializer = keras.initializers.he_normal(seed=19),\n",
    "        recurrent_initializer = keras.initializers.he_normal(seed=19), kernel_regularizer = keras.regularizers.l1(0.01),\n",
    "        recurrent_regularizer = keras.regularizers.l2(0.01))(x_i)\n",
    "x = Dropout(0.4)(x)\n",
    "\n",
    "#b1\n",
    "x_pa = CuDNNLSTM(units=neurons, kernel_initializer = keras.initializers.he_normal(seed=19),\n",
    "        recurrent_initializer = keras.initializers.he_normal(seed=19), kernel_regularizer = keras.regularizers.l1(0.01),\n",
    "        recurrent_regularizer = keras.regularizers.l2(0.01))(x)\n",
    "x = Activation('relu')(x_pa)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "#b2\n",
    "x_pa2 = Dense(neurons, kernel_initializer = keras.initializers.he_normal(seed=19),\n",
    "             bias_initializer = keras.initializers.he_normal(seed=19),\n",
    "             kernel_regularizer = keras.regularizers.l1(0.01),\n",
    "             bias_regularizer = keras.regularizers.l1(0.01))(x_i2)\n",
    "x2 = Activation('relu')(x_pa2)\n",
    "x2 = Dropout(0.1)(x2)\n",
    "x_dot = Dot(axes=1, normalize=True)([x2, x]) \n",
    "\n",
    "# Dot gives 75% test axes=1, normalize=True\n",
    "# Add 33% Mul got stuck at 75% Max got stuck 70%\n",
    "# Average stuck 70%\n",
    "\n",
    "x_add = Concatenate()([x_dot, x_pa])\n",
    "x_f = Activation('relu')(x_add)\n",
    "\n",
    "x_add2 = Concatenate()([x_dot, x_pa2])\n",
    "x_f2 = Activation('relu')(x_add2)\n",
    "\n",
    "x_dot2 = Dot(axes=1, normalize=True)([x_f, x_f2])\n",
    "\n",
    "x_add = Concatenate()([x_dot2, x_pa])\n",
    "x_f = Activation('relu')(x_add)\n",
    "\n",
    "x_add2 = Concatenate()([x_dot2, x_pa2])\n",
    "x_f2 = Activation('relu')(x_add2)\n",
    "\n",
    "x_dot3 = Dot(axes=1, normalize=True)([x_f, x_f2])\n",
    "\n",
    "x_add = Concatenate()([x_dot3, x_pa])\n",
    "x_f = Activation('relu')(x_add)\n",
    "\n",
    "x_add2 = Concatenate()([x_dot3, x_pa2])\n",
    "x_f2 = Activation('relu')(x_add2)\n",
    "\n",
    "x_dot4 = Dot(axes=1, normalize=True)([x_f, x_f2])\n",
    "\n",
    "x_add = Concatenate()([x_dot4, x_pa])\n",
    "x_f = Activation('relu')(x_add)\n",
    "\n",
    "x_add2 = Concatenate()([x_dot4, x_pa2])\n",
    "x_f2 = Activation('relu')(x_add2)\n",
    "\n",
    "x_dot5 = Dot(axes=1, normalize=True)([x_f, x_f2])\n",
    "\n",
    "x_add = Concatenate()([x_dot5, x_i2])\n",
    "x_f = Activation('relu')(x_add)\n",
    "\n",
    "x_f = Dense(128, activation='relu', kernel_initializer = keras.initializers.he_normal(seed=19),\n",
    "             bias_initializer = keras.initializers.he_normal(seed=19),\n",
    "             kernel_regularizer = keras.regularizers.l1(0.01),\n",
    "             bias_regularizer = keras.regularizers.l1(0.01))(x_f)\n",
    "\n",
    "\n",
    "x = Dropout(0.2)(x_f)\n",
    "x_o = Dense(n_class, activation='softmax')(x)\n",
    "\n",
    "\n",
    "model = Model([x_i, x_i2], x_o)\n",
    "\n",
    "# checkpoint\n",
    "filepath=\"dot_res_lstm-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint, lrate]\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 905, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_3 (CuDNNLSTM)        (None, 905, 63)      16632       input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 905, 63)      0           cu_dnnlstm_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 63)           693         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_4 (CuDNNLSTM)        (None, 63)           32256       dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 63)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 63)           0           cu_dnnlstm_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 63)           0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 63)           0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dot_11 (Dot)                    (None, 1)            0           dropout_11[0][0]                 \n",
      "                                                                 dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 64)           0           dot_11[0][0]                     \n",
      "                                                                 cu_dnnlstm_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 64)           0           dot_11[0][0]                     \n",
      "                                                                 dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 64)           0           concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 64)           0           concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dot_12 (Dot)                    (None, 1)            0           activation_25[0][0]              \n",
      "                                                                 activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 64)           0           dot_12[0][0]                     \n",
      "                                                                 cu_dnnlstm_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 64)           0           dot_12[0][0]                     \n",
      "                                                                 dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 64)           0           concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 64)           0           concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dot_13 (Dot)                    (None, 1)            0           activation_27[0][0]              \n",
      "                                                                 activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 64)           0           dot_13[0][0]                     \n",
      "                                                                 cu_dnnlstm_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 64)           0           dot_13[0][0]                     \n",
      "                                                                 dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 64)           0           concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 64)           0           concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dot_14 (Dot)                    (None, 1)            0           activation_29[0][0]              \n",
      "                                                                 activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 64)           0           dot_14[0][0]                     \n",
      "                                                                 cu_dnnlstm_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 64)           0           dot_14[0][0]                     \n",
      "                                                                 dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 64)           0           concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 64)           0           concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dot_15 (Dot)                    (None, 1)            0           activation_31[0][0]              \n",
      "                                                                 activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 11)           0           dot_15[0][0]                     \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 11)           0           concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 128)          1536        activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 128)          0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 2)            258         dropout_12[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 51,375\n",
      "Trainable params: 51,375\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 840 samples, validate on 210 samples\n",
      "Epoch 1/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 2.2784 - acc: 0.7583 - val_loss: 2.4965 - val_acc: 0.7571\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.77619\n",
      "Epoch 2/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 2.5235 - acc: 0.7619 - val_loss: 2.3869 - val_acc: 0.7667\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.77619\n",
      "Epoch 3/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 2.3543 - acc: 0.7679 - val_loss: 1.9938 - val_acc: 0.7619\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.77619\n",
      "Epoch 4/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 2.0412 - acc: 0.7500 - val_loss: 2.1205 - val_acc: 0.7143\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.77619\n",
      "Epoch 5/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 2.1101 - acc: 0.7488 - val_loss: 1.9919 - val_acc: 0.7762\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.77619\n",
      "Epoch 6/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 1.8542 - acc: 0.7524 - val_loss: 1.6051 - val_acc: 0.7571\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.77619\n",
      "Epoch 7/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 1.6585 - acc: 0.7583 - val_loss: 1.7238 - val_acc: 0.7381\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.77619\n",
      "Epoch 8/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 1.6938 - acc: 0.7500 - val_loss: 1.5269 - val_acc: 0.7762\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.77619 to 0.77619, saving model to dot_res_lstm-08-0.78.hdf5\n",
      "Epoch 9/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 1.4549 - acc: 0.7619 - val_loss: 1.3350 - val_acc: 0.7714\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.77619\n",
      "Epoch 10/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 1.3394 - acc: 0.7464 - val_loss: 1.2678 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.77619 to 0.78095, saving model to dot_res_lstm-10-0.78.hdf5\n",
      "Epoch 11/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 1.3201 - acc: 0.7607 - val_loss: 1.3720 - val_acc: 0.7429\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.78095\n",
      "Epoch 12/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 1.2387 - acc: 0.7452 - val_loss: 1.1444 - val_acc: 0.7381\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.78095\n",
      "Epoch 13/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 1.1231 - acc: 0.7500 - val_loss: 1.0843 - val_acc: 0.7714\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.78095\n",
      "Epoch 14/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 1.0968 - acc: 0.7595 - val_loss: 1.0410 - val_acc: 0.7762\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.78095\n",
      "Epoch 15/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.9819 - acc: 0.7702 - val_loss: 0.9272 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.78095 to 0.78095, saving model to dot_res_lstm-15-0.78.hdf5\n",
      "Epoch 16/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.9266 - acc: 0.7655 - val_loss: 0.9188 - val_acc: 0.7762\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.78095\n",
      "Epoch 17/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.9016 - acc: 0.7655 - val_loss: 0.8704 - val_acc: 0.7857\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.78095 to 0.78571, saving model to dot_res_lstm-17-0.79.hdf5\n",
      "Epoch 18/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.8466 - acc: 0.7714 - val_loss: 0.8073 - val_acc: 0.7667\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.78571\n",
      "Epoch 19/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.8150 - acc: 0.7560 - val_loss: 0.7723 - val_acc: 0.7857\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.78571\n",
      "Epoch 20/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.8096 - acc: 0.7536 - val_loss: 0.8020 - val_acc: 0.7476\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.78571\n",
      "Epoch 21/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.7704 - acc: 0.7571 - val_loss: 0.7206 - val_acc: 0.7952\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.78571 to 0.79524, saving model to dot_res_lstm-21-0.80.hdf5\n",
      "Epoch 22/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.7313 - acc: 0.7607 - val_loss: 0.7250 - val_acc: 0.7857\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.79524\n",
      "Epoch 23/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.7158 - acc: 0.7667 - val_loss: 0.6922 - val_acc: 0.7905\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.79524\n",
      "Epoch 24/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.6825 - acc: 0.7631 - val_loss: 0.6532 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.79524\n",
      "Epoch 25/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.6569 - acc: 0.7702 - val_loss: 0.6484 - val_acc: 0.7762\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.79524\n",
      "Epoch 26/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.6450 - acc: 0.7726 - val_loss: 0.6432 - val_acc: 0.7762\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.79524\n",
      "Epoch 27/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.6333 - acc: 0.7726 - val_loss: 0.6111 - val_acc: 0.7762\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.79524\n",
      "Epoch 28/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.6186 - acc: 0.7631 - val_loss: 0.6108 - val_acc: 0.7714\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.79524\n",
      "Epoch 29/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.6103 - acc: 0.7738 - val_loss: 0.6077 - val_acc: 0.7762\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.79524\n",
      "Epoch 30/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5985 - acc: 0.7750 - val_loss: 0.5886 - val_acc: 0.7762\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.79524\n",
      "Epoch 31/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5863 - acc: 0.7702 - val_loss: 0.5791 - val_acc: 0.7762\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.79524\n",
      "Epoch 32/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5846 - acc: 0.7702 - val_loss: 0.5795 - val_acc: 0.7762\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.79524\n",
      "Epoch 33/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5758 - acc: 0.7655 - val_loss: 0.5671 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.79524\n",
      "Epoch 34/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5739 - acc: 0.7714 - val_loss: 0.5631 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.79524\n",
      "Epoch 35/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5620 - acc: 0.7631 - val_loss: 0.5657 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.79524\n",
      "Epoch 36/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5577 - acc: 0.7750 - val_loss: 0.5510 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.79524\n",
      "Epoch 37/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5485 - acc: 0.7726 - val_loss: 0.5441 - val_acc: 0.7762\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.79524\n",
      "Epoch 38/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5544 - acc: 0.7619 - val_loss: 0.5510 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.79524\n",
      "Epoch 39/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5537 - acc: 0.7667 - val_loss: 0.5445 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.79524\n",
      "Epoch 40/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5422 - acc: 0.7655 - val_loss: 0.5396 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.79524\n",
      "Epoch 41/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5364 - acc: 0.7702 - val_loss: 0.5379 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.79524\n",
      "Epoch 42/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5357 - acc: 0.7786 - val_loss: 0.5333 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.79524\n",
      "Epoch 43/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5329 - acc: 0.7810 - val_loss: 0.5308 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.79524\n",
      "Epoch 44/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5368 - acc: 0.7714 - val_loss: 0.5313 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.79524\n",
      "Epoch 45/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5287 - acc: 0.7738 - val_loss: 0.5259 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.79524\n",
      "Epoch 46/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5269 - acc: 0.7774 - val_loss: 0.5254 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.79524\n",
      "Epoch 47/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5385 - acc: 0.7655 - val_loss: 0.5247 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.79524\n",
      "Epoch 48/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5228 - acc: 0.7690 - val_loss: 0.5216 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.79524\n",
      "Epoch 49/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5289 - acc: 0.7655 - val_loss: 0.5210 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.79524\n",
      "Epoch 50/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5227 - acc: 0.7702 - val_loss: 0.5205 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.79524\n",
      "Epoch 51/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5314 - acc: 0.7655 - val_loss: 0.5189 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.79524\n",
      "Epoch 52/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5288 - acc: 0.7679 - val_loss: 0.5185 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.79524\n",
      "Epoch 53/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5265 - acc: 0.7726 - val_loss: 0.5188 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.79524\n",
      "Epoch 54/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5189 - acc: 0.7738 - val_loss: 0.5179 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.79524\n",
      "Epoch 55/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5218 - acc: 0.7714 - val_loss: 0.5177 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.79524\n",
      "Epoch 56/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5236 - acc: 0.7702 - val_loss: 0.5176 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.79524\n",
      "Epoch 57/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5229 - acc: 0.7774 - val_loss: 0.5169 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.79524\n",
      "Epoch 58/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5244 - acc: 0.7726 - val_loss: 0.5167 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.79524\n",
      "Epoch 59/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5093 - acc: 0.7774 - val_loss: 0.5170 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.79524\n",
      "Epoch 60/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5282 - acc: 0.7726 - val_loss: 0.5164 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.79524\n",
      "Epoch 61/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5121 - acc: 0.7798 - val_loss: 0.5163 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.79524\n",
      "Epoch 62/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5177 - acc: 0.7726 - val_loss: 0.5163 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.79524\n",
      "Epoch 63/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5270 - acc: 0.7702 - val_loss: 0.5159 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.79524\n",
      "Epoch 64/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5116 - acc: 0.7762 - val_loss: 0.5159 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.79524\n",
      "Epoch 65/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5180 - acc: 0.7798 - val_loss: 0.5160 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.79524\n",
      "Epoch 66/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5183 - acc: 0.7762 - val_loss: 0.5158 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.79524\n",
      "Epoch 67/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5180 - acc: 0.7726 - val_loss: 0.5157 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.79524\n",
      "Epoch 68/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5264 - acc: 0.7714 - val_loss: 0.5159 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.79524\n",
      "Epoch 69/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5216 - acc: 0.7750 - val_loss: 0.5158 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.79524\n",
      "Epoch 70/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5119 - acc: 0.7798 - val_loss: 0.5157 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.79524\n",
      "Epoch 71/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5170 - acc: 0.7714 - val_loss: 0.5158 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.79524\n",
      "Epoch 72/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5139 - acc: 0.7869 - val_loss: 0.5156 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.79524\n",
      "Epoch 73/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5132 - acc: 0.7786 - val_loss: 0.5156 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.79524\n",
      "Epoch 74/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5246 - acc: 0.7738 - val_loss: 0.5157 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.79524\n",
      "Epoch 75/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5132 - acc: 0.7750 - val_loss: 0.5156 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.79524\n",
      "Epoch 76/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5271 - acc: 0.7595 - val_loss: 0.5156 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.79524\n",
      "Epoch 77/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5325 - acc: 0.7619 - val_loss: 0.5156 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.79524\n",
      "Epoch 78/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5225 - acc: 0.7690 - val_loss: 0.5155 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.79524\n",
      "Epoch 79/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5173 - acc: 0.7738 - val_loss: 0.5155 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.79524\n",
      "Epoch 80/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5109 - acc: 0.7798 - val_loss: 0.5154 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.79524\n",
      "Epoch 81/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5088 - acc: 0.7798 - val_loss: 0.5154 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.79524\n",
      "Epoch 82/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5184 - acc: 0.7750 - val_loss: 0.5154 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.79524\n",
      "Epoch 83/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5153 - acc: 0.7762 - val_loss: 0.5154 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.79524\n",
      "Epoch 84/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5190 - acc: 0.7786 - val_loss: 0.5154 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.79524\n",
      "Epoch 85/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5132 - acc: 0.7786 - val_loss: 0.5154 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.79524\n",
      "Epoch 86/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5204 - acc: 0.7702 - val_loss: 0.5154 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.79524\n",
      "Epoch 87/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5167 - acc: 0.7750 - val_loss: 0.5153 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.79524\n",
      "Epoch 88/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5151 - acc: 0.7714 - val_loss: 0.5153 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.79524\n",
      "Epoch 89/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5205 - acc: 0.7738 - val_loss: 0.5153 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.79524\n",
      "Epoch 90/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5257 - acc: 0.7726 - val_loss: 0.5153 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.79524\n",
      "Epoch 91/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5191 - acc: 0.7726 - val_loss: 0.5153 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.79524\n",
      "Epoch 92/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5166 - acc: 0.7714 - val_loss: 0.5153 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.79524\n",
      "Epoch 93/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5243 - acc: 0.7690 - val_loss: 0.5153 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.79524\n",
      "Epoch 94/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5192 - acc: 0.7750 - val_loss: 0.5153 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.79524\n",
      "Epoch 95/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5165 - acc: 0.7738 - val_loss: 0.5153 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.79524\n",
      "Epoch 96/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5246 - acc: 0.7798 - val_loss: 0.5153 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.79524\n",
      "Epoch 97/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5205 - acc: 0.7714 - val_loss: 0.5153 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.79524\n",
      "Epoch 98/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5100 - acc: 0.7810 - val_loss: 0.5153 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.79524\n",
      "Epoch 99/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5219 - acc: 0.7726 - val_loss: 0.5153 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.79524\n",
      "Epoch 100/100\n",
      "840/840 [==============================] - 2s 2ms/step - loss: 0.5151 - acc: 0.7738 - val_loss: 0.5153 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.79524\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit([Xs_train, Xf_train], y_train, epochs = 100, batch_size = 64, validation_data=([Xs_test, Xf_test], y_test),\n",
    "        callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs_test_t = np.load('Xs_test.npy')\n",
    "Xf_test_t = np.load('Xf_test.npy')\n",
    "y_test_t = np.load('y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.956279878616333, 0.685]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([Xs_test_t, Xf_test_t], y_test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210/210 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5972099741299947, 0.7714285731315613]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([Xs_test, Xf_test], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7743589743589744, 0.9869281045751634, 0.867816091954023, None)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "y_pred = model.predict([Xs_test, Xf_test])\n",
    "y_pred = y_pred.argmax(axis=-1)\n",
    "y_true = y_b[test_index]\n",
    "precision_recall_fscore_support(y_true, y_pred, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
